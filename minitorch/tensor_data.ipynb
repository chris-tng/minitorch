{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from .operators import prod\n",
    "from numpy import array, float64, ndarray\n",
    "import numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DIMS = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexingError(RuntimeError):\n",
    "    \"Exception raised for indexing errors.\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_position(index, strides):\n",
    "    \"\"\"\n",
    "    Converts a multidimensional tensor `index` into a single-dimensional position in\n",
    "    storage based on strides.\n",
    "\n",
    "    Args:\n",
    "       index (array-like): index tuple of ints\n",
    "       strides (array-like): tensor strides\n",
    "\n",
    "    Return:\n",
    "        int : position in storage\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement for Task 2.1.\n",
    "    raise NotImplementedError('Need to implement for Task 2.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(position, shape, out_index):\n",
    "    \"\"\"\n",
    "    Convert a `position` to an index in the `shape`.\n",
    "    Should ensure that enumerating position 0 ... size of a\n",
    "    tensor produces every index exactly once. It\n",
    "    may not be the inverse of `index_to_position`.\n",
    "\n",
    "    Args:\n",
    "       position (int): current position\n",
    "       shape (tuple): tensor shape\n",
    "       out_index (array): the index corresponding to position\n",
    "\n",
    "    Returns:\n",
    "       None : Fills in `index`.\n",
    "\n",
    "    \"\"\"\n",
    "    # TODO: Implement for Task 2.1.\n",
    "    raise NotImplementedError('Need to implement for Task 2.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def broadcast_index(big_index, big_shape, shape, out_index):\n",
    "    \"\"\"\n",
    "    Convert an index into a position (see `index_to_position`),\n",
    "    when the index is from a broadcasted shape. In this case\n",
    "    it may be larger or with more dimensions than the `shape`\n",
    "    given. Additional dimensions may need to be mapped to 0 or\n",
    "    removed.\n",
    "\n",
    "    Args:\n",
    "       big_index (array-like): multidimensional index of bigger tensor\n",
    "       big_shape (array-like): tensor shape of bigger tensor\n",
    "       shape (array-like): tensor shape of smaller tensor\n",
    "       out_index (array-like): multidimensional index of smaller tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement for Task 2.4.\n",
    "    raise NotImplementedError('Need to implement for Task 2.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_broadcast(shape1, shape2):\n",
    "    \"\"\"\n",
    "    Broadcast two shapes to create a new union shape.\n",
    "\n",
    "    Args:\n",
    "       shape1 (tuple): first shape\n",
    "       shape2 (tuple): second shape\n",
    "\n",
    "    Returns:\n",
    "       tuple: broadcasted shape\n",
    "\n",
    "    \"\"\"\n",
    "    # TODO: Implement for Task 2.4.\n",
    "    raise NotImplementedError('Need to implement for Task 2.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strides_from_shape(shape):\n",
    "    layout = [1]\n",
    "    offset = 1\n",
    "    for s in reversed(shape):\n",
    "        layout.append(s * offset)\n",
    "        offset = s * offset\n",
    "    return tuple(reversed(layout[:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `TensorData`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://minitorch.github.io/tensordata.html\n",
    "\n",
    "> To make our code a bit cleaner, we will separate out the internal tensor data from the user-facing tensor. In addition to the shape, `minitorch.TensorData` manages tensor storage and strides.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorData:\n",
    "    def __init__(self, storage, shape, strides=None):\n",
    "        if isinstance(storage, ndarray):\n",
    "            self._storage = storage\n",
    "        else:\n",
    "            self._storage = array(storage, dtype=float64)\n",
    "\n",
    "        if strides is None:\n",
    "            strides = strides_from_shape(shape)\n",
    "\n",
    "        assert isinstance(strides, tuple), \"Strides must be tuple\"\n",
    "        assert isinstance(shape, tuple), \"Shape must be tuple\"\n",
    "        if len(strides) != len(shape):\n",
    "            raise IndexingError(f\"Len of strides {strides} must match {shape}.\")\n",
    "        self._strides = array(strides)\n",
    "        self._shape = array(shape)\n",
    "        self.strides = strides\n",
    "        self.dims = len(strides)\n",
    "        self.size = int(prod(shape))\n",
    "        self.shape = shape\n",
    "        assert len(self._storage) == self.size\n",
    "\n",
    "    def to_cuda_(self):\n",
    "        if not numba.cuda.is_cuda_array(self._storage):\n",
    "            self._storage = numba.cuda.to_device(self._storage)\n",
    "\n",
    "    def is_contiguous(self):\n",
    "        \"Check that the layout is contiguous, i.e. outer dimensions have bigger strides than inner dimensions. \"\n",
    "        last = 1e9\n",
    "        for stride in self._strides:\n",
    "            if stride > last:\n",
    "                return False\n",
    "            last = stride\n",
    "        return True\n",
    "\n",
    "    @staticmethod\n",
    "    def shape_broadcast(shape_a, shape_b):\n",
    "        return shape_broadcast(shape_a, shape_b)\n",
    "\n",
    "    def index(self, index):\n",
    "        if isinstance(index, int):\n",
    "            index = array([index])\n",
    "        if isinstance(index, tuple):\n",
    "            index = array(index)\n",
    "\n",
    "        # Check for errors\n",
    "        if index.shape[0] != len(self.shape):\n",
    "            raise IndexingError(f\"Index {index} must be size of {self.shape}.\")\n",
    "        for i, ind in enumerate(index):\n",
    "            if ind >= self.shape[i]:\n",
    "                raise IndexingError(f\"Index {index} out of range {self.shape}.\")\n",
    "            if ind < 0:\n",
    "                raise IndexingError(f\"Negative indexing for {index} not supported.\")\n",
    "\n",
    "        # Call fast indexing.\n",
    "        return index_to_position(array(index), self._strides)\n",
    "\n",
    "    def indices(self):\n",
    "        lshape = array(self.shape)\n",
    "        out_index = array(self.shape)\n",
    "        for i in range(self.size):\n",
    "            count(i, lshape, out_index)\n",
    "            yield tuple(out_index)\n",
    "\n",
    "    def sample(self):\n",
    "        return tuple((random.randint(0, s - 1) for s in self.shape))\n",
    "\n",
    "    def get(self, key):\n",
    "        return self._storage[self.index(key)]\n",
    "\n",
    "    def set(self, key, val):\n",
    "        self._storage[self.index(key)] = val\n",
    "\n",
    "    def tuple(self):\n",
    "        return (self._storage, self._shape, self._strides)\n",
    "\n",
    "    def permute(self, *order):\n",
    "        \"\"\"\n",
    "        Permute the dimensions of the tensor.\n",
    "\n",
    "        Args:\n",
    "           order (list): a permutation of the dimensions\n",
    "\n",
    "        Returns:\n",
    "           :class:`TensorData`: a new TensorData with the same storage and a new dimension order.\n",
    "        \"\"\"\n",
    "        assert list(sorted(order)) == list(\n",
    "            range(len(self.shape))\n",
    "        ), f\"Must give a position to each dimension. Shape: {self.shape} Order: {order}\"\n",
    "\n",
    "        # TODO: Implement for Task 2.1.\n",
    "        raise NotImplementedError('Need to implement for Task 2.1')\n",
    "\n",
    "    def to_string(self):\n",
    "        s = \"\"\n",
    "        for index in self.indices():\n",
    "            l = \"\"\n",
    "            for i in range(len(index) - 1, -1, -1):\n",
    "                if index[i] == 0:\n",
    "                    l = \"\\n%s[\" % (\"\\t\" * i) + l\n",
    "                else:\n",
    "                    break\n",
    "            s += l\n",
    "            v = self.get(index)\n",
    "            s += f\"{v:3.2f}\"\n",
    "            l = \"\"\n",
    "            for i in range(len(index) - 1, -1, -1):\n",
    "                if index[i] == self.shape[i] - 1:\n",
    "                    l += \"]\"\n",
    "                else:\n",
    "                    break\n",
    "            if l:\n",
    "                s += l\n",
    "            else:\n",
    "                s += \" \"\n",
    "        return s"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "py:light,ipynb"
  },
  "kernelspec": {
   "display_name": "minitorch",
   "language": "python",
   "name": "minitorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
