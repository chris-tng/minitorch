{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of the core Tensor object for autodifferentiation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://minitorch.github.io/tensordata.html\n",
    "\n",
    "> A tensor is a multi-dimensional array of arbitrary dimensions\n",
    "\n",
    "`Tensor` is the convenient abstraction while the underlying storage is `TensorData` which is always an 1D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from .autodiff import FunctionBase, Variable\n",
    "from . import operators\n",
    "import random\n",
    "from .tensor_ops import TensorOps\n",
    "from .tensor_data import TensorData\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction\n",
    "def zeros(shape):\n",
    "    return Tensor.make([0] * int(operators.prod(shape)), shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand(shape):\n",
    "    \"\"\"\n",
    "    Produce a random tensor of size `shape`.\n",
    "\n",
    "    Args:\n",
    "       shape (tuple): shape of tensor.\n",
    "\n",
    "    Returns:\n",
    "       :class:`Tensor` : New tensor\n",
    "    \"\"\"\n",
    "    vals = [random.random() for _ in range(int(operators.prod(shape)))]\n",
    "    return Tensor.make(vals, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor(ls, shape=None):\n",
    "    if not shape:\n",
    "        shape = (len(ls),)\n",
    "    return Tensor.make(ls, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_fromlist(ls):\n",
    "    def shape(ls):\n",
    "        if isinstance(ls, list):\n",
    "            return [len(ls)] + shape(ls[0])\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def flatten(ls):\n",
    "        if isinstance(ls, list):\n",
    "            return [y for x in ls for y in flatten(x)]\n",
    "        else:\n",
    "            return [ls]\n",
    "\n",
    "    cur = flatten(ls)\n",
    "    shape = shape(ls)\n",
    "    return tensor(cur, tuple(shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor class\n",
    "class Tensor(Variable):\n",
    "    def __init__(self, v, back=None, name=None, backend=None):\n",
    "        assert isinstance(v, TensorData)\n",
    "        super().__init__(back, name=name)\n",
    "        self._tensor = v\n",
    "        self.tf = backend\n",
    "        if backend is None:\n",
    "            self.tf = TensorFunctions\n",
    "\n",
    "    def _new(self, tensor_data):\n",
    "        return Tensor(tensor_data, backend=self.tf)\n",
    "\n",
    "    @staticmethod\n",
    "    def make(storage, shape, strides=None, backend=None):\n",
    "        return Tensor(TensorData(storage, shape, strides), backend=backend)\n",
    "\n",
    "    def type_(self, tf):\n",
    "        self.tf = tf\n",
    "        if \"Cuda\" in str(tf._backend):\n",
    "            self._tensor.to_cuda_()\n",
    "\n",
    "    # Properties\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self._tensor.shape\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return self._tensor.size\n",
    "\n",
    "    @property\n",
    "    def dims(self):\n",
    "        return self._tensor.dims\n",
    "\n",
    "    def to_numpy(self):\n",
    "        return self.contiguous()._tensor._storage.reshape(self.shape)\n",
    "\n",
    "    def contiguous(self):\n",
    "        return self.tf.Copy.apply(self)\n",
    "\n",
    "    def ensure_tensor(self, b):\n",
    "        if isinstance(b, (int, float)):\n",
    "            b = tensor([b])\n",
    "        b.type_(self.tf)\n",
    "        return b\n",
    "\n",
    "    # Functions\n",
    "    def __add__(self, b):\n",
    "        return self.tf.Add.apply(self, self.ensure_tensor(b))\n",
    "\n",
    "    def __sub__(self, b):\n",
    "        return self.tf.Add.apply(self, -self.ensure_tensor(b))\n",
    "\n",
    "    def __mul__(self, b):\n",
    "        return self.tf.Mul.apply(self, self.ensure_tensor(b))\n",
    "\n",
    "    def __truediv__(self, b):\n",
    "        return self.tf.Mul.apply(self, self.tf.Inv.apply(self.ensure_tensor(b)))\n",
    "\n",
    "    def __lt__(self, b):\n",
    "        return self.tf.LT.apply(self, self.ensure_tensor(b))\n",
    "\n",
    "    def __eq__(self, b):\n",
    "        return self.tf.EQ.apply(self, self.ensure_tensor(b))\n",
    "\n",
    "    def __gt__(self, b):\n",
    "        return self.tf.LT.apply(self.ensure_tensor(b), self)\n",
    "\n",
    "    def __neg__(self):\n",
    "        return self.tf.Neg.apply(self)\n",
    "\n",
    "    def sigmoid(self):\n",
    "        return self.tf.Sigmoid.apply(self)\n",
    "\n",
    "    def relu(self):\n",
    "        return self.tf.ReLU.apply(self)\n",
    "\n",
    "    def log(self):\n",
    "        return self.tf.Log.apply(self)\n",
    "\n",
    "    def exp(self):\n",
    "        return self.tf.Exp.apply(self)\n",
    "\n",
    "    def sum(self, dim=None):\n",
    "        return self.tf.Sum.apply(self, dim)\n",
    "\n",
    "    def mean(self, dim=None):\n",
    "        return self.tf.Mean.apply(self, dim)\n",
    "\n",
    "    def permute(self, *order):\n",
    "        return self.tf.Permute.apply(self, order)\n",
    "\n",
    "    def view(self, *shape):\n",
    "        return self.tf.View.apply(self, shape)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self._tensor.to_string()\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self._tensor.get(key)\n",
    "\n",
    "    def __setitem__(self, key, val):\n",
    "        self._tensor.set(key, val)\n",
    "\n",
    "    @property\n",
    "    def grad(self):\n",
    "        return self.derivative\n",
    "\n",
    "    def expand(self, other):\n",
    "        \"\"\n",
    "        if self.shape == other.shape:\n",
    "            return other\n",
    "\n",
    "        shape = TensorData.shape_broadcast(self.shape, other.shape)\n",
    "        buf = zeros(shape)\n",
    "        self.tf._id_map(other, out=buf)\n",
    "        if self.shape == shape:\n",
    "            return buf\n",
    "\n",
    "        buf2 = zeros(self.shape)\n",
    "        self.tf._add_reduce(buf, out=buf2)\n",
    "        return buf2\n",
    "\n",
    "    # Internal\n",
    "    def zeros(self, shape=None):\n",
    "\n",
    "        if shape is None:\n",
    "            out = zeros(self.shape)\n",
    "        else:\n",
    "            out = zeros(shape)\n",
    "        out.type_(self.tf)\n",
    "        return out\n",
    "\n",
    "    def tuple(self):\n",
    "        return self._tensor.tuple()\n",
    "\n",
    "    # Extra\n",
    "    def get_data(self):\n",
    "        return Tensor(self._tensor, backend=self.tf)\n",
    "\n",
    "    def backward(self, grad_output=None):\n",
    "        if grad_output is None:\n",
    "            assert self.shape == (1,), \"Must provide grad_output if non-scalar\"\n",
    "            grad_output = tensor([1.0])\n",
    "            grad_output.tf = self.tf\n",
    "        super().backward(grad_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructors\n",
    "class Function(FunctionBase):\n",
    "    data_type = Tensor\n",
    "\n",
    "    @staticmethod\n",
    "    def variable(data, back):\n",
    "        t = Tensor(data[0], back)\n",
    "        t.type_(data[1])\n",
    "        return t\n",
    "\n",
    "    @staticmethod\n",
    "    def data(a):\n",
    "        return (a._tensor, a.tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tensor_functions(backend):\n",
    "    neg_map = backend.map(operators.neg)\n",
    "    sigmoid_map = backend.map(operators.sigmoid)\n",
    "    relu_map = backend.map(operators.relu)\n",
    "    log_map = backend.map(operators.log)\n",
    "    exp_map = backend.map(operators.exp)\n",
    "    id_map = backend.map(operators.id)\n",
    "    inv_map = backend.map(operators.inv)\n",
    "\n",
    "    add_zip = backend.zip(operators.add)\n",
    "    mul_zip = backend.zip(operators.mul)\n",
    "    lt_zip = backend.zip(operators.lt)\n",
    "    eq_zip = backend.zip(operators.eq)\n",
    "    relu_back_zip = backend.zip(operators.relu_back)\n",
    "    log_back_zip = backend.zip(operators.log_back)\n",
    "    inv_back_zip = backend.zip(operators.inv_back)\n",
    "\n",
    "    add_reduce = backend.reduce(operators.add)\n",
    "\n",
    "    class TF:\n",
    "        _add_reduce = add_reduce\n",
    "        _id_map = id_map\n",
    "        _backend = backend\n",
    "\n",
    "        class Neg(Function):\n",
    "            @staticmethod\n",
    "            def forward(ctx, t1):\n",
    "                return neg_map(t1)\n",
    "\n",
    "            @staticmethod\n",
    "            def backward(ctx, grad_output):\n",
    "                return neg_map(grad_output)\n",
    "\n",
    "        class Inv(Function):\n",
    "            @staticmethod\n",
    "            def forward(ctx, t1):\n",
    "                ctx.save_for_backward(t1)\n",
    "                return inv_map(t1)\n",
    "\n",
    "            @staticmethod\n",
    "            def backward(ctx, grad_output):\n",
    "                t1 = ctx.saved_values\n",
    "                return inv_back_zip(t1, grad_output)\n",
    "\n",
    "        class Add(Function):\n",
    "            @staticmethod\n",
    "            def forward(ctx, t1, t2):\n",
    "                return add_zip(t1, t2)\n",
    "\n",
    "            @staticmethod\n",
    "            def backward(ctx, grad_output):\n",
    "                return grad_output, grad_output\n",
    "\n",
    "        class Mul(Function):\n",
    "            @staticmethod\n",
    "            def forward(ctx, a, b):\n",
    "                # TODO: Implement for Task 2.2.\n",
    "                raise NotImplementedError('Need to implement for Task 2.2')\n",
    "\n",
    "            @staticmethod\n",
    "            def backward(ctx, grad_output):\n",
    "                # TODO: Implement for Task 2.3.\n",
    "                raise NotImplementedError('Need to implement for Task 2.3')\n",
    "\n",
    "        class Sigmoid(Function):\n",
    "            @staticmethod\n",
    "            def forward(ctx, a):\n",
    "                # TODO: Implement for Task 2.2.\n",
    "                raise NotImplementedError('Need to implement for Task 2.2')\n",
    "\n",
    "            @staticmethod\n",
    "            def backward(ctx, grad_output):\n",
    "                # TODO: Implement for Task 2.3.\n",
    "                raise NotImplementedError('Need to implement for Task 2.3')\n",
    "\n",
    "        class ReLU(Function):\n",
    "            @staticmethod\n",
    "            def forward(ctx, a):\n",
    "                # TODO: Implement for Task 2.2.\n",
    "                raise NotImplementedError('Need to implement for Task 2.2')\n",
    "\n",
    "            @staticmethod\n",
    "            def backward(ctx, grad_output):\n",
    "                # TODO: Implement for Task 2.3.\n",
    "                raise NotImplementedError('Need to implement for Task 2.3')\n",
    "\n",
    "        class Log(Function):\n",
    "            @staticmethod\n",
    "            def forward(ctx, a):\n",
    "                # TODO: Implement for Task 2.2.\n",
    "                raise NotImplementedError('Need to implement for Task 2.2')\n",
    "\n",
    "            @staticmethod\n",
    "            def backward(ctx, grad_output):\n",
    "                # TODO: Implement for Task 2.3.\n",
    "                raise NotImplementedError('Need to implement for Task 2.3')\n",
    "\n",
    "        class Exp(Function):\n",
    "            @staticmethod\n",
    "            def forward(ctx, a):\n",
    "                # TODO: Implement for Task 2.2.\n",
    "                raise NotImplementedError('Need to implement for Task 2.2')\n",
    "\n",
    "            @staticmethod\n",
    "            def backward(ctx, grad_output):\n",
    "                # TODO: Implement for Task 2.3.\n",
    "                raise NotImplementedError('Need to implement for Task 2.3')\n",
    "\n",
    "        class Sum(Function):\n",
    "            @staticmethod\n",
    "            def forward(ctx, a, dim):\n",
    "                ctx.save_for_backward(a.shape)\n",
    "                if dim is not None:\n",
    "                    return add_reduce(a, [dim])\n",
    "                else:\n",
    "                    return add_reduce(a, list(range(a.dims))).view(1)\n",
    "\n",
    "            @staticmethod\n",
    "            def backward(ctx, grad_output):\n",
    "                return grad_output\n",
    "\n",
    "        class Mean(Function):\n",
    "            @staticmethod\n",
    "            def forward(ctx, a, dim):\n",
    "                # TODO: Implement for Task 2.2.\n",
    "                raise NotImplementedError('Need to implement for Task 2.2')\n",
    "\n",
    "            @staticmethod\n",
    "            def backward(ctx, grad_output):\n",
    "                # TODO: Implement for Task 2.3.\n",
    "                raise NotImplementedError('Need to implement for Task 2.3')\n",
    "\n",
    "        class LT(Function):\n",
    "            @staticmethod\n",
    "            def forward(ctx, a, b):\n",
    "                # TODO: Implement for Task 2.2.\n",
    "                raise NotImplementedError('Need to implement for Task 2.2')\n",
    "\n",
    "            @staticmethod\n",
    "            def backward(ctx, grad_output):\n",
    "                # TODO: Implement for Task 2.3.\n",
    "                raise NotImplementedError('Need to implement for Task 2.3')\n",
    "\n",
    "        class EQ(Function):\n",
    "            @staticmethod\n",
    "            def forward(ctx, a, b):\n",
    "                # TODO: Implement for Task 2.2.\n",
    "                raise NotImplementedError('Need to implement for Task 2.2')\n",
    "\n",
    "            @staticmethod\n",
    "            def backward(ctx, grad_output):\n",
    "                # TODO: Implement for Task 2.3.\n",
    "                raise NotImplementedError('Need to implement for Task 2.3')\n",
    "\n",
    "        class Permute(Function):\n",
    "            @staticmethod\n",
    "            def forward(ctx, a, order):\n",
    "                ctx.save_for_backward(order)\n",
    "                return a._new(a._tensor.permute(*order))\n",
    "\n",
    "            @staticmethod\n",
    "            def backward(ctx, grad_output):\n",
    "                order = ctx.saved_values\n",
    "                order = [a[0] for a in sorted(enumerate(order), key=lambda a: a[1])]\n",
    "                return grad_output._new(grad_output._tensor.permute(*order))\n",
    "\n",
    "        class View(Function):\n",
    "            @staticmethod\n",
    "            def forward(ctx, a, shape):\n",
    "                ctx.save_for_backward(a.shape)\n",
    "                assert a._tensor.is_contiguous, \"Must be contiguous to view\"\n",
    "                t = Tensor.make(a._tensor._storage, shape)\n",
    "                t.type_(a.tf)\n",
    "                return t\n",
    "\n",
    "            @staticmethod\n",
    "            def backward(ctx, grad_output):\n",
    "                original = ctx.saved_values\n",
    "                ret = Tensor.make(grad_output._tensor._storage, original)\n",
    "                ret.type_(grad_output.tf)\n",
    "                return ret\n",
    "\n",
    "        class Copy(Function):\n",
    "            @staticmethod\n",
    "            def forward(ctx, a):\n",
    "                return id_map(a)\n",
    "\n",
    "            @staticmethod\n",
    "            def backward(ctx, grad_output):\n",
    "                return grad_output\n",
    "\n",
    "    return TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TensorFunctions = make_tensor_functions(TensorOps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment for Module 3\n",
    "CudaTensorFunctions = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "from cuda_ops import CudaOps\n",
    "CudaTensorFunctions = make_tensor_functions(CudaOps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def central_difference(f, *vals, arg=0, epsilon=1e-6, ind=None):\n",
    "    x = vals[arg]\n",
    "    up = zeros(x.shape)\n",
    "    up[ind] = epsilon\n",
    "    vals1 = [x if j != arg else x + up for j, x in enumerate(vals)]\n",
    "    vals2 = [x if j != arg else x - up for j, x in enumerate(vals)]\n",
    "    delta = f(*vals1).sum() - f(*vals2).sum()\n",
    "\n",
    "    return delta[0] / (2.0 * epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_check(f, *vals):\n",
    "    for x in vals:\n",
    "        x.requires_grad_(True)\n",
    "        x.zero_grad_()\n",
    "    random.seed(10)\n",
    "    out = f(*vals)\n",
    "    out.sum().backward()\n",
    "\n",
    "    for i, x in enumerate(vals):\n",
    "        ind = x._tensor.sample()\n",
    "        check = central_difference(f, *vals, arg=i, ind=ind)\n",
    "        np.testing.assert_allclose(x.grad[ind], check, 1e-2, 1e-2)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "py:light,ipynb"
  },
  "kernelspec": {
   "display_name": "minitorch",
   "language": "python",
   "name": "minitorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
