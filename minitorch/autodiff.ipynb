{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from .util import wrap_tuple, unwrap_tuple\n",
    "except:\n",
    "    from util import wrap_tuple, unwrap_tuple\n",
    "\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nx = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### `Context`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Context:\n",
    "    \"\"\"\n",
    "    Simple storage to keep values in the forward pass that are required \n",
    "    in backward pass.\n",
    "    Used by History\n",
    "    \n",
    "    Attributes:\n",
    "        no_grad (bool)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, no_grad: bool=False):\n",
    "        self._saved_values = None\n",
    "        self.no_grad = no_grad\n",
    "\n",
    "    def save_for_backward(self, *values):\n",
    "        if self.no_grad:\n",
    "            return\n",
    "        self._saved_values = values\n",
    "\n",
    "    @property\n",
    "    def saved_values(self):\n",
    "        assert not self.no_grad, \"Doesn't require grad\"\n",
    "        assert self._saved_values is not None, \"Did you forget to save values?\"\n",
    "        return unwrap_tuple(self._saved_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `FunctionBase`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FunctionBase:\n",
    "    \"\"\"\n",
    "    A function that can act on :class:`Variable` arguments to\n",
    "    produce a :class:`Variable` output, while tracking the internal history.\n",
    "\n",
    "    Subclass needs to implement\n",
    "    - forward: perform the forward pass\n",
    "    - backward: perform the backward pass\n",
    "    - variable: attach a :class:`Variable` as output\n",
    "    \n",
    "    Base class uses `apply` method to wrap subclass's `forward`\n",
    "\n",
    "    Call by :func:`FunctionBase.apply`.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def variable(raw, history):\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def apply(cls, *vals):\n",
    "        \"\"\"\n",
    "        Wrapper around `forward` of subclass to\n",
    "        - attach :class:`Context` object as storage of `forward` pass\n",
    "        \"\"\"\n",
    "        raw_vals = []\n",
    "        need_grad = False\n",
    "        # unpack Variable to \"raw\" values\n",
    "        for v in vals:\n",
    "            if isinstance(v, Variable):\n",
    "                if v.history is not None:\n",
    "                    need_grad = True\n",
    "                raw_vals.append(v.get_data())\n",
    "            else:\n",
    "                raw_vals.append(v)\n",
    "        ctx = Context(not need_grad)\n",
    "        c = cls.forward(ctx, *raw_vals)\n",
    "        assert isinstance(c, cls.data_type), \"Expected return typ %s got %s\" % (\n",
    "            cls.data_type,\n",
    "            type(c),\n",
    "        )\n",
    "        back = None\n",
    "        if need_grad:\n",
    "            back = History(cls, ctx, vals)\n",
    "        return cls.variable(cls.data(c), back)\n",
    "\n",
    "    @classmethod\n",
    "    def chain_rule(cls, ctx, inputs, d_output) -> List[\"VariableWithDeriv\"]:\n",
    "        \"\"\"\n",
    "        Implement the derivative chain-rule.\n",
    "        Used by `backpropagate` <- `Variable.backward()`\n",
    "\n",
    "        Args:\n",
    "            cls (:class:`FunctionBase`): The function\n",
    "            ctx (:class:`Context`) : The context from running forward\n",
    "            inputs (list of args) : The args that were passed to :func:`FunctionBase.apply` (e.g. :math:`x, y`)\n",
    "            d_output (number) : The `d_output` value in the chain rule.\n",
    "\n",
    "        Returns:\n",
    "            list of :class:`VariableWithDeriv`: A list of variables with their derivatives\n",
    "            for each :class:`Variable` object in input (other inputs should be ignored)\n",
    "        \"\"\"\n",
    "        d_vars = cls.backward(ctx, d_output)\n",
    "        return [VariableWithDeriv(var, d_var) for var, d_var in zip(inputs, d_vars) \n",
    "                if isinstance(var, Variable)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### `History`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class History:\n",
    "    \"\"\"\n",
    "    `History` stores the last `Function` operations that were used to\n",
    "    construct an autodiff object.\n",
    "\n",
    "    Attributes:\n",
    "        last_fn (:class:`FunctionBase`) : The last function that was called.\n",
    "        ctx (:class:`Context`): The context for that function.\n",
    "        inputs (list of inputs) : The inputs that were given when `last_fn.forward` was called.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, last_fn: FunctionBase=None, ctx: Context=None, \n",
    "                 inputs: Optional[List]=None):\n",
    "        self.last_fn = last_fn\n",
    "        self.ctx = ctx\n",
    "        self.inputs = inputs\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return self.last_fn is None\n",
    "\n",
    "    def chain_rule(self, d_output):\n",
    "        return self.last_fn.chain_rule(self.ctx, self.inputs, d_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Variable`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`Variable` consists of\n",
    "- `History` : \n",
    "    - last func `FunctionBase` - similar to `grad_fn` in pytorch\n",
    "    - ctx: `Context`      \n",
    "    - inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    x ----- [ Mul ] ------> z \n",
    "               |\n",
    "    y ---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "the goal is `auto-differentiation` : abstraction is computation graph where\n",
    "- nodes are operations\n",
    "- edges are tensors flowing in and out of nodes\n",
    "\n",
    "`forward` pass is simple\n",
    "\n",
    "`backward` pass: \n",
    "- starting from z: z needs to keep track of the op that creates it: link from z to Mul\n",
    "- `z.backward(d_out or 1.0)` : send the upstream grad to Mul\n",
    "\n",
    "- at `Mul`: notice the following\n",
    "$$ \\frac{dz}{dx} = y $$\n",
    "$$ \\frac{dz}{dy} = x $$\n",
    "\n",
    "this means we need to save x, y for backward pass. \n",
    "\n",
    "- `Mul` need to perform the gradient computation with respect to all of its inputs, this means it needs to keep track all of its inputs\n",
    "- `Mul.backward()` performs the computation needed and returns the grads wrt inputs\n",
    "- `Mul.backward()` recursively asks its inputs to backward the gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Potential Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Option 1: implement the computation as instance method\n",
    "\n",
    "```python\n",
    "class Mul:\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._inputs = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        self._inputs = [x, y]\n",
    "        return x * y\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        x, y = self._inputs\n",
    "        return d_out * y, d_out * x\n",
    "```\n",
    "\n",
    "- Option 2: implement the computation as class method. For this, we can not keep a reference to the input, defer this to an object called `History`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable:\n",
    "    \"\"\"\n",
    "    \n",
    "    Attributes:\n",
    "        history (:class:`History`) : The sequence of function calls that led to this variable.\n",
    "        derivative (number): The derivative with respect to this variable.\n",
    "        name (string) : an optional name for debugging.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, history: \"History\", name: str = None):\n",
    "        assert history is None or isinstance(history, History), history\n",
    "        self.history = history\n",
    "        self._derivative = None\n",
    "\n",
    "        # For debugging can have a name.\n",
    "        if name is not None:\n",
    "            self.name = name\n",
    "        else:\n",
    "            self.name = str(uuid.uuid4())\n",
    "\n",
    "    def requires_grad_(self, val):\n",
    "        self.history = History(None, None, None)\n",
    "\n",
    "    def backward(self, d_output=None):\n",
    "        \"\"\"\n",
    "        Calls autodiff to fill in the derivatives for the history of this object.\n",
    "        \"\"\"\n",
    "        if d_output is None:\n",
    "            d_output = 1.0\n",
    "        backpropagate(VariableWithDeriv(self, d_output))\n",
    "\n",
    "    @property\n",
    "    def derivative(self):\n",
    "        return self._derivative\n",
    "\n",
    "    ## IGNORE\n",
    "    def __hash__(self):\n",
    "        return hash(self._name)\n",
    "\n",
    "    def _add_deriv(self, val):\n",
    "        assert self.history.is_leaf(), \"Only leaf variables can have derivatives.\"\n",
    "        assert type(val) == float, f\"Only supports scalar float so far\"\n",
    "        if self._derivative is None:\n",
    "            self._derivative = self.zeros()\n",
    "        self._derivative += val\n",
    "\n",
    "    def zero_grad_(self):\n",
    "        self._derivative = self.zeros()\n",
    "\n",
    "    def __radd__(self, b):\n",
    "        return self + b\n",
    "\n",
    "    def __rmul__(self, b):\n",
    "        return self * b\n",
    "\n",
    "    def zeros(self):\n",
    "        return 0.0\n",
    "\n",
    "    def expand(self, x):\n",
    "        return x\n",
    "\n",
    "    # def make_graph(self, graphfile, grad_output=1.0):\n",
    "    #     global nx\n",
    "    #     import networkx as nx\n",
    "    #     import graphviz\n",
    "\n",
    "    #     G = AutodiffRunner().run(self, grad_output, make_graph=True)\n",
    "    #     nx.nx_pydot.write_dot(G, graphfile)\n",
    "    #     graphviz.render(filepath=\"graph.dot\", format=\"png\", engine=\"dot\")\n",
    "\n",
    "    ## IGNORE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### `Variable with Derivative`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Intuitively, this class represents the `backward` tensor (which is the Variable and its upstream derivative) while the Variable class represents the `forward` tensor.\n",
    "\n",
    "`backprop` is a multi-step process where at each step, `VariableWithDeriv` holds the backward tensor to an op, the op performs the grad computation and returns the output backward tensor.\n",
    "\n",
    "`forward`\n",
    "\n",
    "            forward\n",
    "    x ----> [ Mul ] ----> z\n",
    "               |\n",
    "    y ---------|\n",
    "    \n",
    "`backward`\n",
    "\n",
    "                             backward\n",
    "    x <---- (x, y*dL/dz) --- [ Mul ] <---- (z, dL/dz) --- z\n",
    "                                |\n",
    "    y <---- (y, x*dL/dz) -------|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class VariableWithDeriv:\n",
    "    \"Holder for a variable with it derivative.\"\n",
    "\n",
    "    def __init__(self, variable, deriv):\n",
    "        self.variable = variable\n",
    "        self.deriv = variable.expand(deriv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def is_leaf(val):\n",
    "    return isinstance(val, Variable) and val.history.is_leaf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Backprop`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Potential Impl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- recursive (local) `backprop` which has the tensor to call backward on the op, the op in turn calls backward on the input tensor. The limitation of this approach is that it's a local procedure, it doesn't have a global view to optimize the computation\n",
    "\n",
    "- static global `backprop`: has the potential to optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Breath-first search` : https://en.wikipedia.org/wiki/Breadth-first_search\n",
    "\n",
    "> Breadth-first search (BFS) is an algorithm for traversing or searching tree or graph data structures. It starts at the tree root (or some arbitrary node of a graph, sometimes referred to as a 'search key'[1]), and explores all of the neighbor nodes at the present depth prior to moving on to the nodes at the next depth level.\n",
    "\n",
    "> It uses the opposite strategy as depth-first search, which instead explores the node branch as far as possible before being forced to backtrack and expand other nodes\n",
    "\n",
    "initiated by `Var.backward`\n",
    "\n",
    "- why it uses `VariableWithDeriv`, see the section `VariableWithDeriv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def backpropagate(final_variable_with_deriv: VariableWithDeriv):\n",
    "    \"\"\"\n",
    "    Runs a breadth-first search on the computation graph in order to\n",
    "    propagate derivative to the leaves.\n",
    "\n",
    "    See :doc:`backpropagate` for details on the algorithm\n",
    "\n",
    "    Args:\n",
    "       final_variable_with_deriv (:class:`VariableWithDeriv`): The final value\n",
    "           and its derivative that we want to propagate backward to the leaves.\n",
    "    \"\"\"\n",
    "    q = OrderedDict(\n",
    "        {final_variable_with_deriv.variable.name: final_variable_with_deriv}\n",
    "    )\n",
    "    while len(q) > 0:\n",
    "        _, v = q.popitem(last=False) # pop the first item\n",
    "        var, deriv = v.variable, v.deriv\n",
    "        if is_leaf(var): # no history, can't backprop further\n",
    "            var._add_deriv(deriv)\n",
    "        else:\n",
    "            vars_with_deriv: List[VariableWithDeriv] = var.history.chain_rule(deriv)\n",
    "            for in_var in vars_with_deriv:\n",
    "                if in_var.variable.name in q:\n",
    "                    # if already in queue, accumulate the grad\n",
    "                    q[in_var.variable.name].deriv += in_var.deriv\n",
    "                else:\n",
    "                    # if not, append to the queue\n",
    "                    q.update({in_var.variable.name: in_var})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('v1', <__main__.Variable object at 0x7fbdd2bc71d0>), ('v2', <__main__.Variable object at 0x7fbdd2bc73d0>)])\n",
      "OrderedDict([('v1', <__main__.Variable object at 0x7fbdd2bc71d0>), ('v2', <__main__.Variable object at 0x7fbdd2bc73d0>), ('v3', <__main__.Variable object at 0x7fbdd2bc7c50>)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('v1', <__main__.Variable at 0x7fbdd2bc71d0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using orderdict as a LIFO queue\n",
    "v1 = Variable(None, name=\"v1\")\n",
    "v2 = Variable(None, name=\"v2\")\n",
    "\n",
    "d = OrderedDict({v.name: v for v in [v1, v2]})\n",
    "\n",
    "print(d)\n",
    "\n",
    "v3 = Variable(None, name=\"v3\")\n",
    "d.update({v3.name: v3})\n",
    "\n",
    "print(d)\n",
    "\n",
    "d.popitem(last=False)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "minitorch",
   "language": "python",
   "name": "minitorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
